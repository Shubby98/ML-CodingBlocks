{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4637\n",
      "[['Dan', 'Morgan', 'told', 'himself', 'he', 'would', 'forget', 'Ann', 'Turner', '.'], ['He', 'was', 'well', 'rid', 'of', 'her', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "data = brown.sents(categories=['adventure'])\n",
    "print(len(data))\n",
    "type(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dan Morgan told himself he would forget Ann Turner .\n",
      "He was well rid of her .\n",
      "He certainly didn't want a wife who was fickle as Ann .\n",
      "If he had married her , he'd have been asking for trouble .\n",
      "But all of this was rationalization .\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(' '.join(data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dan Morgan told himself he would forget Ann Turner .He was well rid of her .'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(data[0])+ ' '.join(data[1])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dan Morgan told himself he would forget Ann Turner .He was well rid of her .']\n"
     ]
    }
   ],
   "source": [
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RegexpTokenizer' object has no attribute 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-bae86c51d703>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_word_except\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'RegexpTokenizer' object has no attribute 'tokenizer'"
     ]
    }
   ],
   "source": [
    "reg_exp = \"[a-zA-Z]+\"\n",
    "all_word_except = \"[]+\"\n",
    "tokenizer = RegexpTokenizer(all_word_except)\n",
    "\n",
    "tokenizer.tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sent,stopwords):\n",
    "    useful_words = [w for w in sent if w not in stopwords]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = remove_stopwords(text,stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D', 'n', ' ', 'M', 'r', 'g', 'n', ' ', 'l', ' ', 'h', 'e', 'l', 'f', ' ', 'h', 'e', ' ', 'w', 'u', 'l', ' ', 'f', 'r', 'g', 'e', ' ', 'A', 'n', 'n', ' ', 'T', 'u', 'r', 'n', 'e', 'r', ' ', '.', 'H', 'e', ' ', 'w', ' ', 'w', 'e', 'l', 'l', ' ', 'r', ' ', 'f', ' ', 'h', 'e', 'r', ' ', '.']\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer, PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "ss = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = LancasterStemmer()\n",
    "ps = PorterStemmer()\n",
    "wn = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learn'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls.stem('learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'story'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemmatize('stories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Akhilesh anti-Muslim, Mulayam in cahoots with BJP: Mayawati',\n",
    "    'Xiaomi officially discontinues Mi Max and Mi Note series'\n",
    "    'One nation, one election? Perish the thought',\n",
    "    'J&K: 4 terrorists killed by security forces in Shopian encounter',\n",
    "    'Hypertension treatment may slow down Alzheimers progression'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_data = cv.fit_transform(corpus).toarray()\n",
    "vectorized_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['akhilesh', 'anti', 'bjp', 'cahoots', 'in', 'mayawati', 'mulayam',\n",
       "        'muslim', 'with'],\n",
       "       dtype='<U12')]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.inverse_transform(vectorized_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'akhilesh': 0, 'anti': 3, 'muslim': 20, 'mulayam': 19, 'in': 13, 'cahoots': 6, 'with': 35, 'bjp': 4, 'mayawati': 17, 'xiaomi': 36, 'officially': 23, 'discontinues': 7, 'mi': 18, 'max': 15, 'and': 2, 'note': 22, 'seriesone': 28, 'nation': 21, 'one': 24, 'election': 9, 'perish': 25, 'the': 32, 'thought': 33, 'terrorists': 31, 'killed': 14, 'by': 5, 'security': 27, 'forces': 11, 'shopian': 29, 'encounter': 10, 'hypertension': 12, 'treatment': 34, 'may': 16, 'slow': 30, 'down': 8, 'alzheimers': 1, 'progression': 26}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "myTokenizer = RegexpTokenizer(\"[a-zA-Z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=myTokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "tf = TfidfVectorizer()\n",
    "vd = tf.fit_transform(corpus).toarray()\n",
    "print(np.argmax(vd[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'akhilesh': 0,\n",
       " 'anti': 3,\n",
       " 'muslim': 20,\n",
       " 'mulayam': 19,\n",
       " 'in': 13,\n",
       " 'cahoots': 6,\n",
       " 'with': 35,\n",
       " 'bjp': 4,\n",
       " 'mayawati': 17,\n",
       " 'xiaomi': 36,\n",
       " 'officially': 23,\n",
       " 'discontinues': 7,\n",
       " 'mi': 18,\n",
       " 'max': 15,\n",
       " 'and': 2,\n",
       " 'note': 22,\n",
       " 'seriesone': 28,\n",
       " 'nation': 21,\n",
       " 'one': 24,\n",
       " 'election': 9,\n",
       " 'perish': 25,\n",
       " 'the': 32,\n",
       " 'thought': 33,\n",
       " 'terrorists': 31,\n",
       " 'killed': 14,\n",
       " 'by': 5,\n",
       " 'security': 27,\n",
       " 'forces': 11,\n",
       " 'shopian': 29,\n",
       " 'encounter': 10,\n",
       " 'hypertension': 12,\n",
       " 'treatment': 34,\n",
       " 'may': 16,\n",
       " 'slow': 30,\n",
       " 'down': 8,\n",
       " 'alzheimers': 1,\n",
       " 'progression': 26}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "newsgroup = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR', 'description'])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroup.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = newsgroup['data'][21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : alt.atheism\n",
      "1 : comp.graphics\n",
      "2 : comp.os.ms-windows.misc\n",
      "3 : comp.sys.ibm.pc.hardware\n",
      "4 : comp.sys.mac.hardware\n",
      "5 : comp.windows.x\n",
      "6 : misc.forsale\n",
      "7 : rec.autos\n",
      "8 : rec.motorcycles\n",
      "9 : rec.sport.baseball\n",
      "10 : rec.sport.hockey\n",
      "11 : sci.crypt\n",
      "12 : sci.electronics\n",
      "13 : sci.med\n",
      "14 : sci.space\n",
      "15 : soc.religion.christian\n",
      "16 : talk.politics.guns\n",
      "17 : talk.politics.mideast\n",
      "18 : talk.politics.misc\n",
      "19 : talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "for i, name in enumerate(newsgroup['target_names']):\n",
    "    print(i, \":\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 4, 4, ..., 3, 1, 8])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroup['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = newsgroup.data\n",
    "y = newsgroup.target\n",
    "label = newsgroup.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From: leunggm@odin.control.utoronto.ca (Gary Leung)',\n",
       " 'Subject: Re: NHL Team Captains',\n",
       " 'Organization: University of Toronto, Systems Control Group',\n",
       " 'Lines: 20',\n",
       " '',\n",
       " 'In article <1993Apr20.151818.4319@samba.oit.unc.edu> Scott.Marks@launchpad.unc.edu (Scott Marks) writes:',\n",
       " '>>And of course, Mike Ramsey was (at one time) the captain in Buffalo prior to',\n",
       " '>>being traded to Pittsburgh.  Currently, the Penguins have 3 former captains',\n",
       " \">>and 1 real captain (Lemieux) playing for them.  They rotate the A's during the\",\n",
       " '>>season (and even the C while Mario was out).  Even Troy Loney has worn the C',\n",
       " '>>for the Pens.',\n",
       " '>',\n",
       " '',\n",
       " 'I think that Mike Foligno was the captain of the Sabres when he',\n",
       " \"got traded to the Leafs. Also, wasn't Rick Vaive the captain of\",\n",
       " 'the Leafs when he got traded to Chicago (with Steve Thomas for',\n",
       " 'Ed Olcyzk and someone). Speaking of the Leafs, I believe that',\n",
       " 'Darryl Sittler was their captain (he\\'d torn the \"C\" off his',\n",
       " 'jersey but I think he re-claimed the captaincy later on) when he',\n",
       " 'was traded to the Flyers.',\n",
       " '',\n",
       " 'Oh yeah, of course, Gretzky was the captain of the Oilers before',\n",
       " \"he was traded wasn't he? \",\n",
       " '',\n",
       " 'Gary',\n",
       " '']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
